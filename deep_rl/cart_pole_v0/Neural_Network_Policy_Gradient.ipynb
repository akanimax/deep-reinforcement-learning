{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for estimating policy trained using policy_gradient method\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "This notebook solves the cart-pole task using a neural network and trained using the policy_gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/animesh/Programming/Envs/deep_learning/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function to play one episode of the cart-pole game and return the corresponding states and actions. Note that, for playing the game, we are using the neural network to take actions :). Yes! it's like inception. The neural network plays the game to get better at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please don't mind the long function names :( I am sort of habitual to it ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_environment = lambda: gym.make('CartPole-v0')  # create a lambda function for obtaining the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_cart_pole_episode(model, dis_gamma=0.1):\n",
    "    \"\"\"\n",
    "        play single episode of the cart-pole game in order to generate the learning data\n",
    "        @args:\n",
    "            model: neural network object used to predict the action\n",
    "        @returns:\n",
    "            experience => (states, returns): tuple of lists of state and return (**Not reward)\n",
    "    \"\"\"\n",
    "    # obtain the cart_pole environment\n",
    "    env = get_environment()\n",
    "    \n",
    "    # reset environment to obtain the first set of observations\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # initialize the states and rewards lists\n",
    "    states = [obs]\n",
    "    rewards = []  # note that initial state has no reward corresponding to it\n",
    "    \n",
    "    # play the game untill it lasts\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_probs = model.predict(obs)\n",
    "        action = np.random.choice(range(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        # take the action on the environment\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # append the state and reward to appropriate lists\n",
    "        states.append(obs)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    # now that we have the rewards, calculate the returns from them\n",
    "    # Note that return for a state is the \n",
    "    # Expected value of rewards (here onwards) discounted by the discount factor \\gamma\n",
    "    # G(t) = r + (gamma * G(t + 1))\n",
    "    \n",
    "    # initialize the returns list **Note that the last state has a return of 0\n",
    "    returns = [0]\n",
    "    \n",
    "    # calculate the returns in reverse order since it is efficient to do so\n",
    "    while reward in reversed(rewards):\n",
    "        returns.append(reward + (gamma * returns[-1]))\n",
    "        \n",
    "    # reverse the returns list\n",
    "    returns = reversed(returns)\n",
    "        \n",
    "    # return the calculated lists\n",
    "    return states, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
