""" Run the simulation of the multi-armed bandits
    *** This looks very similar to the EpsilonGreedy,
    but there are changes in the for loop. Take a look
    Besides, there is no epsilon required.
"""

import numpy as np

from ExploreExploit.OptimisticInitialValues.bandit import Bandit


def simulate(*means, initial_optimistic_mean=100, n=10000, feed_back=True, feed_back_factor=100):
    """
    Run the simulation of the multi-armed bandit problem using the Epsilon_Greedy strategy

    args:
        *means: means for the bandits in the experimentation
            the number of mean arguments passed will determine
            the number of bandits

        initial_optimistic_mean: The optimistic initial value for all the bandits

        n: number of iterations to simulate

        feed_back: printing feedback required or not

        feed_back_factor: number of iterations after which feedback message is displayed

    return:
        Cumulative_Average: List of average rewards received during the simulation
    """
    if n < 0:
        raise ValueError("Value of 'n' cannot be negative")

    data = []  # initialize to empty array
    bandits = [Bandit(m, initial_optimistic_mean) for m in means]
    print("Running the experiment ... ")
    for cnt in range(n):
        current_best = np.argmax([b.mean for b in bandits])
        bandit = bandits[current_best]

        # pull the bandit's arm and update the mean estimate
        reward = bandit.pull()
        bandit.update(reward)
        data.append(reward)

        if feed_back and (cnt+1) % feed_back_factor == 0:
            print("Ran %d iterations ..." % (cnt+1))

    # obtain the cumulative averages from the rewards
    cumulative_averages = np.cumsum(data) / np.arange(1, n+1)

    print("Experiment complete ...")

    # return the cumulative averages
    return cumulative_averages
